{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ZvSi585JB_Ac"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset , DataLoader\n",
        "from sklearn.model_selection import train_test_split\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SpamDetect(nn.Module):\n",
        "  def __init__(self , input_features):\n",
        "    super().__init__()\n",
        "    self.linear_1 = nn.Sequential(\n",
        "         nn.Linear(in_features= input_features , out_features = 128),\n",
        "         nn.ReLU(),\n",
        "         nn.Linear(128 , 64),\n",
        "         nn.Linear(64 ,1)\n",
        "    )\n",
        "    self.layer_2 = nn.Sequential(\n",
        "        nn.Sigmoid()\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.layer_2(self.linear_1(x))\n"
      ],
      "metadata": {
        "id": "VSTOPzJUGJ5H"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df = pd.read_csv(\"hf://datasets/thehamkercat/telegram-spam-ham/dataset.csv\")\n",
        "\n",
        "df.columns = ['label', 'text']\n",
        "\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3PtzfzLuN31J",
        "outputId": "fc6f888c-3674-4cb1-a42c-924010db4209"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  label                                               text\n",
            "0  spam  naturally irresistible your corporate identity...\n",
            "1  spam  the stock trading gunslinger fanny is merrill ...\n",
            "2  spam  unbelievable new homes made easy im wanting to...\n",
            "3  spam  4 color printing special request additional in...\n",
            "4  spam  do not have money get software cds from here s...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Convert labels to binary (spam = 1, ham = 0)\n",
        "df['label'] = df['label'].map({'spam': 1, 'ham': 0})\n",
        "\n",
        "print(df)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['text'], df['label'], test_size=0.2, random_state=42)\n",
        "\n",
        "print(f'Training samples: {len(X_train)}, Testing samples: {len(X_test)}')\n"
      ],
      "metadata": {
        "id": "yvdpBUGWRdf6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4c83504-aa42-4e26-f729-7737fab2fc65"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       label                                               text\n",
            "0          1  naturally irresistible your corporate identity...\n",
            "1          1  the stock trading gunslinger fanny is merrill ...\n",
            "2          1  unbelievable new homes made easy im wanting to...\n",
            "3          1  4 color printing special request additional in...\n",
            "4          1  do not have money get software cds from here s...\n",
            "...      ...                                                ...\n",
            "20343      0                                               /ban\n",
            "20344      0                                               /ban\n",
            "20345      0                                               /ban\n",
            "20346      0                                          Kaisi hii\n",
            "20347      0                                            Shock q\n",
            "\n",
            "[20348 rows x 2 columns]\n",
            "Training samples: 16278, Testing samples: 4070\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Convert text data to numerical data\n",
        "vectorizer = CountVectorizer()\n",
        "X_train = vectorizer.fit_transform(X_train).toarray()\n",
        "X_test = vectorizer.transform(X_test).toarray()\n",
        "\n",
        "print(f'Feature vector shape: {X_train.shape}')\n"
      ],
      "metadata": {
        "id": "ZA7sDeEpRieC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d3d065b-8c56-4dc7-c0a5-28034170f7d3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature vector shape: (16278, 53450)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SpamDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = torch.tensor(X, dtype=torch.float32)\n",
        "        self.y = torch.tensor(y.values, dtype=torch.float32)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n"
      ],
      "metadata": {
        "id": "yqnAiN9xFZbA"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create dataset objects\n",
        "train_dataset = SpamDataset(X_train, y_train)\n",
        "test_dataset = SpamDataset(X_test, y_test)\n",
        "\n",
        "# Create DataLoader objects\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n"
      ],
      "metadata": {
        "id": "Zi8JVxHUGBeq"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = SpamDetect(input_features = X_train.shape[1])"
      ],
      "metadata": {
        "id": "YF9153ToGBaD"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.BCELoss()\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(),\n",
        "                      lr = 0.1)\n"
      ],
      "metadata": {
        "id": "Rc-c79QnG9Jp"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 3\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  model.train();\n",
        "  epoch_loss = 0\n",
        "  for emails , labels in train_loader:\n",
        "    y_pred = model(emails)\n",
        "    labels = labels.view(-1,1)\n",
        "    loss = loss_fn(y_pred , labels)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "    epoch_loss += loss.item()\n",
        "\n",
        "    if(epoch):\n",
        "      print(f'Epoch [{epoch}, Loss: {epoch_loss/len(train_loader):.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VlP8mv8Gbr7L",
        "outputId": "0c2d0a86-5801-4743-d483-ae990c4d18b1"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1, Loss: 0.0001\n",
            "Epoch [1, Loss: 0.0003\n",
            "Epoch [1, Loss: 0.0005\n",
            "Epoch [1, Loss: 0.0008\n",
            "Epoch [1, Loss: 0.0009\n",
            "Epoch [1, Loss: 0.0010\n",
            "Epoch [1, Loss: 0.0011\n",
            "Epoch [1, Loss: 0.0012\n",
            "Epoch [1, Loss: 0.0013\n",
            "Epoch [1, Loss: 0.0014\n",
            "Epoch [1, Loss: 0.0015\n",
            "Epoch [1, Loss: 0.0015\n",
            "Epoch [1, Loss: 0.0015\n",
            "Epoch [1, Loss: 0.0019\n",
            "Epoch [1, Loss: 0.0021\n",
            "Epoch [1, Loss: 0.0022\n",
            "Epoch [1, Loss: 0.0022\n",
            "Epoch [1, Loss: 0.0022\n",
            "Epoch [1, Loss: 0.0023\n",
            "Epoch [1, Loss: 0.0024\n",
            "Epoch [1, Loss: 0.0026\n",
            "Epoch [1, Loss: 0.0026\n",
            "Epoch [1, Loss: 0.0027\n",
            "Epoch [1, Loss: 0.0028\n",
            "Epoch [1, Loss: 0.0030\n",
            "Epoch [1, Loss: 0.0032\n",
            "Epoch [1, Loss: 0.0033\n",
            "Epoch [1, Loss: 0.0034\n",
            "Epoch [1, Loss: 0.0035\n",
            "Epoch [1, Loss: 0.0037\n",
            "Epoch [1, Loss: 0.0037\n",
            "Epoch [1, Loss: 0.0044\n",
            "Epoch [1, Loss: 0.0044\n",
            "Epoch [1, Loss: 0.0046\n",
            "Epoch [1, Loss: 0.0050\n",
            "Epoch [1, Loss: 0.0060\n",
            "Epoch [1, Loss: 0.0061\n",
            "Epoch [1, Loss: 0.0062\n",
            "Epoch [1, Loss: 0.0062\n",
            "Epoch [1, Loss: 0.0064\n",
            "Epoch [1, Loss: 0.0064\n",
            "Epoch [1, Loss: 0.0066\n",
            "Epoch [1, Loss: 0.0067\n",
            "Epoch [1, Loss: 0.0067\n",
            "Epoch [1, Loss: 0.0068\n",
            "Epoch [1, Loss: 0.0073\n",
            "Epoch [1, Loss: 0.0076\n",
            "Epoch [1, Loss: 0.0077\n",
            "Epoch [1, Loss: 0.0078\n",
            "Epoch [1, Loss: 0.0080\n",
            "Epoch [1, Loss: 0.0081\n",
            "Epoch [1, Loss: 0.0081\n",
            "Epoch [1, Loss: 0.0082\n",
            "Epoch [1, Loss: 0.0084\n",
            "Epoch [1, Loss: 0.0085\n",
            "Epoch [1, Loss: 0.0086\n",
            "Epoch [1, Loss: 0.0087\n",
            "Epoch [1, Loss: 0.0088\n",
            "Epoch [1, Loss: 0.0091\n",
            "Epoch [1, Loss: 0.0091\n",
            "Epoch [1, Loss: 0.0093\n",
            "Epoch [1, Loss: 0.0093\n",
            "Epoch [1, Loss: 0.0094\n",
            "Epoch [1, Loss: 0.0094\n",
            "Epoch [1, Loss: 0.0094\n",
            "Epoch [1, Loss: 0.0095\n",
            "Epoch [1, Loss: 0.0096\n",
            "Epoch [1, Loss: 0.0097\n",
            "Epoch [1, Loss: 0.0098\n",
            "Epoch [1, Loss: 0.0099\n",
            "Epoch [1, Loss: 0.0099\n",
            "Epoch [1, Loss: 0.0101\n",
            "Epoch [1, Loss: 0.0103\n",
            "Epoch [1, Loss: 0.0103\n",
            "Epoch [1, Loss: 0.0104\n",
            "Epoch [1, Loss: 0.0104\n",
            "Epoch [1, Loss: 0.0105\n",
            "Epoch [1, Loss: 0.0109\n",
            "Epoch [1, Loss: 0.0110\n",
            "Epoch [1, Loss: 0.0112\n",
            "Epoch [1, Loss: 0.0112\n",
            "Epoch [1, Loss: 0.0112\n",
            "Epoch [1, Loss: 0.0113\n",
            "Epoch [1, Loss: 0.0114\n",
            "Epoch [1, Loss: 0.0116\n",
            "Epoch [1, Loss: 0.0117\n",
            "Epoch [1, Loss: 0.0118\n",
            "Epoch [1, Loss: 0.0119\n",
            "Epoch [1, Loss: 0.0119\n",
            "Epoch [1, Loss: 0.0120\n",
            "Epoch [1, Loss: 0.0120\n",
            "Epoch [1, Loss: 0.0121\n",
            "Epoch [1, Loss: 0.0122\n",
            "Epoch [1, Loss: 0.0123\n",
            "Epoch [1, Loss: 0.0124\n",
            "Epoch [1, Loss: 0.0125\n",
            "Epoch [1, Loss: 0.0126\n",
            "Epoch [1, Loss: 0.0129\n",
            "Epoch [1, Loss: 0.0130\n",
            "Epoch [1, Loss: 0.0131\n",
            "Epoch [1, Loss: 0.0132\n",
            "Epoch [1, Loss: 0.0133\n",
            "Epoch [1, Loss: 0.0134\n",
            "Epoch [1, Loss: 0.0135\n",
            "Epoch [1, Loss: 0.0135\n",
            "Epoch [1, Loss: 0.0136\n",
            "Epoch [1, Loss: 0.0137\n",
            "Epoch [1, Loss: 0.0137\n",
            "Epoch [1, Loss: 0.0138\n",
            "Epoch [1, Loss: 0.0138\n",
            "Epoch [1, Loss: 0.0140\n",
            "Epoch [1, Loss: 0.0140\n",
            "Epoch [1, Loss: 0.0142\n",
            "Epoch [1, Loss: 0.0145\n",
            "Epoch [1, Loss: 0.0146\n",
            "Epoch [1, Loss: 0.0147\n",
            "Epoch [1, Loss: 0.0149\n",
            "Epoch [1, Loss: 0.0151\n",
            "Epoch [1, Loss: 0.0152\n",
            "Epoch [1, Loss: 0.0152\n",
            "Epoch [1, Loss: 0.0153\n",
            "Epoch [1, Loss: 0.0157\n",
            "Epoch [1, Loss: 0.0159\n",
            "Epoch [1, Loss: 0.0160\n",
            "Epoch [1, Loss: 0.0161\n",
            "Epoch [1, Loss: 0.0163\n",
            "Epoch [1, Loss: 0.0164\n",
            "Epoch [1, Loss: 0.0167\n",
            "Epoch [1, Loss: 0.0167\n",
            "Epoch [1, Loss: 0.0168\n",
            "Epoch [1, Loss: 0.0170\n",
            "Epoch [1, Loss: 0.0171\n",
            "Epoch [1, Loss: 0.0171\n",
            "Epoch [1, Loss: 0.0172\n",
            "Epoch [1, Loss: 0.0172\n",
            "Epoch [1, Loss: 0.0172\n",
            "Epoch [1, Loss: 0.0174\n",
            "Epoch [1, Loss: 0.0174\n",
            "Epoch [1, Loss: 0.0175\n",
            "Epoch [1, Loss: 0.0175\n",
            "Epoch [1, Loss: 0.0177\n",
            "Epoch [1, Loss: 0.0179\n",
            "Epoch [1, Loss: 0.0180\n",
            "Epoch [1, Loss: 0.0181\n",
            "Epoch [1, Loss: 0.0181\n",
            "Epoch [1, Loss: 0.0182\n",
            "Epoch [1, Loss: 0.0182\n",
            "Epoch [1, Loss: 0.0183\n",
            "Epoch [1, Loss: 0.0184\n",
            "Epoch [1, Loss: 0.0184\n",
            "Epoch [1, Loss: 0.0185\n",
            "Epoch [1, Loss: 0.0188\n",
            "Epoch [1, Loss: 0.0189\n",
            "Epoch [1, Loss: 0.0189\n",
            "Epoch [1, Loss: 0.0189\n",
            "Epoch [1, Loss: 0.0192\n",
            "Epoch [1, Loss: 0.0193\n",
            "Epoch [1, Loss: 0.0194\n",
            "Epoch [1, Loss: 0.0194\n",
            "Epoch [1, Loss: 0.0195\n",
            "Epoch [1, Loss: 0.0197\n",
            "Epoch [1, Loss: 0.0198\n",
            "Epoch [1, Loss: 0.0198\n",
            "Epoch [1, Loss: 0.0199\n",
            "Epoch [1, Loss: 0.0201\n",
            "Epoch [1, Loss: 0.0202\n",
            "Epoch [1, Loss: 0.0202\n",
            "Epoch [1, Loss: 0.0207\n",
            "Epoch [1, Loss: 0.0209\n",
            "Epoch [1, Loss: 0.0211\n",
            "Epoch [1, Loss: 0.0212\n",
            "Epoch [1, Loss: 0.0214\n",
            "Epoch [1, Loss: 0.0216\n",
            "Epoch [1, Loss: 0.0218\n",
            "Epoch [1, Loss: 0.0219\n",
            "Epoch [1, Loss: 0.0219\n",
            "Epoch [1, Loss: 0.0224\n",
            "Epoch [1, Loss: 0.0226\n",
            "Epoch [1, Loss: 0.0231\n",
            "Epoch [1, Loss: 0.0235\n",
            "Epoch [1, Loss: 0.0236\n",
            "Epoch [1, Loss: 0.0238\n",
            "Epoch [1, Loss: 0.0240\n",
            "Epoch [1, Loss: 0.0241\n",
            "Epoch [1, Loss: 0.0241\n",
            "Epoch [1, Loss: 0.0242\n",
            "Epoch [1, Loss: 0.0243\n",
            "Epoch [1, Loss: 0.0244\n",
            "Epoch [1, Loss: 0.0245\n",
            "Epoch [1, Loss: 0.0246\n",
            "Epoch [1, Loss: 0.0250\n",
            "Epoch [1, Loss: 0.0253\n",
            "Epoch [1, Loss: 0.0255\n",
            "Epoch [1, Loss: 0.0257\n",
            "Epoch [1, Loss: 0.0261\n",
            "Epoch [1, Loss: 0.0261\n",
            "Epoch [1, Loss: 0.0262\n",
            "Epoch [1, Loss: 0.0263\n",
            "Epoch [1, Loss: 0.0270\n",
            "Epoch [1, Loss: 0.0271\n",
            "Epoch [1, Loss: 0.0275\n",
            "Epoch [1, Loss: 0.0275\n",
            "Epoch [1, Loss: 0.0276\n",
            "Epoch [1, Loss: 0.0276\n",
            "Epoch [1, Loss: 0.0277\n",
            "Epoch [1, Loss: 0.0277\n",
            "Epoch [1, Loss: 0.0280\n",
            "Epoch [1, Loss: 0.0282\n",
            "Epoch [1, Loss: 0.0283\n",
            "Epoch [1, Loss: 0.0286\n",
            "Epoch [1, Loss: 0.0288\n",
            "Epoch [1, Loss: 0.0289\n",
            "Epoch [1, Loss: 0.0292\n",
            "Epoch [1, Loss: 0.0294\n",
            "Epoch [1, Loss: 0.0298\n",
            "Epoch [1, Loss: 0.0298\n",
            "Epoch [1, Loss: 0.0300\n",
            "Epoch [1, Loss: 0.0302\n",
            "Epoch [1, Loss: 0.0302\n",
            "Epoch [1, Loss: 0.0305\n",
            "Epoch [1, Loss: 0.0306\n",
            "Epoch [1, Loss: 0.0306\n",
            "Epoch [1, Loss: 0.0307\n",
            "Epoch [1, Loss: 0.0308\n",
            "Epoch [1, Loss: 0.0313\n",
            "Epoch [1, Loss: 0.0314\n",
            "Epoch [1, Loss: 0.0315\n",
            "Epoch [1, Loss: 0.0318\n",
            "Epoch [1, Loss: 0.0319\n",
            "Epoch [1, Loss: 0.0321\n",
            "Epoch [1, Loss: 0.0323\n",
            "Epoch [1, Loss: 0.0325\n",
            "Epoch [1, Loss: 0.0325\n",
            "Epoch [1, Loss: 0.0326\n",
            "Epoch [1, Loss: 0.0329\n",
            "Epoch [1, Loss: 0.0330\n",
            "Epoch [1, Loss: 0.0332\n",
            "Epoch [1, Loss: 0.0334\n",
            "Epoch [1, Loss: 0.0338\n",
            "Epoch [1, Loss: 0.0340\n",
            "Epoch [1, Loss: 0.0341\n",
            "Epoch [1, Loss: 0.0342\n",
            "Epoch [1, Loss: 0.0343\n",
            "Epoch [1, Loss: 0.0343\n",
            "Epoch [1, Loss: 0.0343\n",
            "Epoch [1, Loss: 0.0348\n",
            "Epoch [1, Loss: 0.0348\n",
            "Epoch [1, Loss: 0.0350\n",
            "Epoch [1, Loss: 0.0351\n",
            "Epoch [1, Loss: 0.0353\n",
            "Epoch [1, Loss: 0.0354\n",
            "Epoch [1, Loss: 0.0355\n",
            "Epoch [1, Loss: 0.0356\n",
            "Epoch [1, Loss: 0.0357\n",
            "Epoch [1, Loss: 0.0358\n",
            "Epoch [2, Loss: 0.0001\n",
            "Epoch [2, Loss: 0.0001\n",
            "Epoch [2, Loss: 0.0002\n",
            "Epoch [2, Loss: 0.0002\n",
            "Epoch [2, Loss: 0.0003\n",
            "Epoch [2, Loss: 0.0006\n",
            "Epoch [2, Loss: 0.0007\n",
            "Epoch [2, Loss: 0.0007\n",
            "Epoch [2, Loss: 0.0008\n",
            "Epoch [2, Loss: 0.0010\n",
            "Epoch [2, Loss: 0.0012\n",
            "Epoch [2, Loss: 0.0013\n",
            "Epoch [2, Loss: 0.0013\n",
            "Epoch [2, Loss: 0.0013\n",
            "Epoch [2, Loss: 0.0016\n",
            "Epoch [2, Loss: 0.0018\n",
            "Epoch [2, Loss: 0.0018\n",
            "Epoch [2, Loss: 0.0020\n",
            "Epoch [2, Loss: 0.0022\n",
            "Epoch [2, Loss: 0.0022\n",
            "Epoch [2, Loss: 0.0023\n",
            "Epoch [2, Loss: 0.0023\n",
            "Epoch [2, Loss: 0.0024\n",
            "Epoch [2, Loss: 0.0025\n",
            "Epoch [2, Loss: 0.0026\n",
            "Epoch [2, Loss: 0.0027\n",
            "Epoch [2, Loss: 0.0027\n",
            "Epoch [2, Loss: 0.0027\n",
            "Epoch [2, Loss: 0.0028\n",
            "Epoch [2, Loss: 0.0028\n",
            "Epoch [2, Loss: 0.0029\n",
            "Epoch [2, Loss: 0.0029\n",
            "Epoch [2, Loss: 0.0030\n",
            "Epoch [2, Loss: 0.0031\n",
            "Epoch [2, Loss: 0.0032\n",
            "Epoch [2, Loss: 0.0033\n",
            "Epoch [2, Loss: 0.0041\n",
            "Epoch [2, Loss: 0.0041\n",
            "Epoch [2, Loss: 0.0044\n",
            "Epoch [2, Loss: 0.0045\n",
            "Epoch [2, Loss: 0.0047\n",
            "Epoch [2, Loss: 0.0048\n",
            "Epoch [2, Loss: 0.0049\n",
            "Epoch [2, Loss: 0.0050\n",
            "Epoch [2, Loss: 0.0052\n",
            "Epoch [2, Loss: 0.0054\n",
            "Epoch [2, Loss: 0.0055\n",
            "Epoch [2, Loss: 0.0058\n",
            "Epoch [2, Loss: 0.0058\n",
            "Epoch [2, Loss: 0.0060\n",
            "Epoch [2, Loss: 0.0061\n",
            "Epoch [2, Loss: 0.0061\n",
            "Epoch [2, Loss: 0.0062\n",
            "Epoch [2, Loss: 0.0062\n",
            "Epoch [2, Loss: 0.0063\n",
            "Epoch [2, Loss: 0.0064\n",
            "Epoch [2, Loss: 0.0066\n",
            "Epoch [2, Loss: 0.0068\n",
            "Epoch [2, Loss: 0.0069\n",
            "Epoch [2, Loss: 0.0072\n",
            "Epoch [2, Loss: 0.0073\n",
            "Epoch [2, Loss: 0.0074\n",
            "Epoch [2, Loss: 0.0075\n",
            "Epoch [2, Loss: 0.0078\n",
            "Epoch [2, Loss: 0.0078\n",
            "Epoch [2, Loss: 0.0079\n",
            "Epoch [2, Loss: 0.0083\n",
            "Epoch [2, Loss: 0.0084\n",
            "Epoch [2, Loss: 0.0087\n",
            "Epoch [2, Loss: 0.0088\n",
            "Epoch [2, Loss: 0.0090\n",
            "Epoch [2, Loss: 0.0090\n",
            "Epoch [2, Loss: 0.0090\n",
            "Epoch [2, Loss: 0.0093\n",
            "Epoch [2, Loss: 0.0093\n",
            "Epoch [2, Loss: 0.0094\n",
            "Epoch [2, Loss: 0.0094\n",
            "Epoch [2, Loss: 0.0095\n",
            "Epoch [2, Loss: 0.0096\n",
            "Epoch [2, Loss: 0.0096\n",
            "Epoch [2, Loss: 0.0097\n",
            "Epoch [2, Loss: 0.0098\n",
            "Epoch [2, Loss: 0.0098\n",
            "Epoch [2, Loss: 0.0099\n",
            "Epoch [2, Loss: 0.0100\n",
            "Epoch [2, Loss: 0.0101\n",
            "Epoch [2, Loss: 0.0102\n",
            "Epoch [2, Loss: 0.0103\n",
            "Epoch [2, Loss: 0.0103\n",
            "Epoch [2, Loss: 0.0110\n",
            "Epoch [2, Loss: 0.0113\n",
            "Epoch [2, Loss: 0.0114\n",
            "Epoch [2, Loss: 0.0115\n",
            "Epoch [2, Loss: 0.0177\n",
            "Epoch [2, Loss: 0.0177\n",
            "Epoch [2, Loss: 0.0179\n",
            "Epoch [2, Loss: 0.0179\n",
            "Epoch [2, Loss: 0.0181\n",
            "Epoch [2, Loss: 0.0185\n",
            "Epoch [2, Loss: 0.0186\n",
            "Epoch [2, Loss: 0.0186\n",
            "Epoch [2, Loss: 0.0186\n",
            "Epoch [2, Loss: 0.0187\n",
            "Epoch [2, Loss: 0.0188\n",
            "Epoch [2, Loss: 0.0190\n",
            "Epoch [2, Loss: 0.0191\n",
            "Epoch [2, Loss: 0.0191\n",
            "Epoch [2, Loss: 0.0192\n",
            "Epoch [2, Loss: 0.0195\n",
            "Epoch [2, Loss: 0.0199\n",
            "Epoch [2, Loss: 0.0200\n",
            "Epoch [2, Loss: 0.0202\n",
            "Epoch [2, Loss: 0.0203\n",
            "Epoch [2, Loss: 0.0206\n",
            "Epoch [2, Loss: 0.0207\n",
            "Epoch [2, Loss: 0.0210\n",
            "Epoch [2, Loss: 0.0211\n",
            "Epoch [2, Loss: 0.0211\n",
            "Epoch [2, Loss: 0.0212\n",
            "Epoch [2, Loss: 0.0212\n",
            "Epoch [2, Loss: 0.0213\n",
            "Epoch [2, Loss: 0.0214\n",
            "Epoch [2, Loss: 0.0214\n",
            "Epoch [2, Loss: 0.0215\n",
            "Epoch [2, Loss: 0.0216\n",
            "Epoch [2, Loss: 0.0216\n",
            "Epoch [2, Loss: 0.0217\n",
            "Epoch [2, Loss: 0.0218\n",
            "Epoch [2, Loss: 0.0218\n",
            "Epoch [2, Loss: 0.0220\n",
            "Epoch [2, Loss: 0.0221\n",
            "Epoch [2, Loss: 0.0223\n",
            "Epoch [2, Loss: 0.0223\n",
            "Epoch [2, Loss: 0.0223\n",
            "Epoch [2, Loss: 0.0225\n",
            "Epoch [2, Loss: 0.0226\n",
            "Epoch [2, Loss: 0.0227\n",
            "Epoch [2, Loss: 0.0228\n",
            "Epoch [2, Loss: 0.0228\n",
            "Epoch [2, Loss: 0.0229\n",
            "Epoch [2, Loss: 0.0229\n",
            "Epoch [2, Loss: 0.0229\n",
            "Epoch [2, Loss: 0.0230\n",
            "Epoch [2, Loss: 0.0230\n",
            "Epoch [2, Loss: 0.0231\n",
            "Epoch [2, Loss: 0.0231\n",
            "Epoch [2, Loss: 0.0233\n",
            "Epoch [2, Loss: 0.0233\n",
            "Epoch [2, Loss: 0.0233\n",
            "Epoch [2, Loss: 0.0236\n",
            "Epoch [2, Loss: 0.0237\n",
            "Epoch [2, Loss: 0.0241\n",
            "Epoch [2, Loss: 0.0245\n",
            "Epoch [2, Loss: 0.0246\n",
            "Epoch [2, Loss: 0.0247\n",
            "Epoch [2, Loss: 0.0248\n",
            "Epoch [2, Loss: 0.0248\n",
            "Epoch [2, Loss: 0.0249\n",
            "Epoch [2, Loss: 0.0250\n",
            "Epoch [2, Loss: 0.0253\n",
            "Epoch [2, Loss: 0.0254\n",
            "Epoch [2, Loss: 0.0258\n",
            "Epoch [2, Loss: 0.0258\n",
            "Epoch [2, Loss: 0.0259\n",
            "Epoch [2, Loss: 0.0259\n",
            "Epoch [2, Loss: 0.0261\n",
            "Epoch [2, Loss: 0.0263\n",
            "Epoch [2, Loss: 0.0264\n",
            "Epoch [2, Loss: 0.0265\n",
            "Epoch [2, Loss: 0.0269\n",
            "Epoch [2, Loss: 0.0274\n",
            "Epoch [2, Loss: 0.0274\n",
            "Epoch [2, Loss: 0.0277\n",
            "Epoch [2, Loss: 0.0280\n",
            "Epoch [2, Loss: 0.0282\n",
            "Epoch [2, Loss: 0.0283\n",
            "Epoch [2, Loss: 0.0283\n",
            "Epoch [2, Loss: 0.0284\n",
            "Epoch [2, Loss: 0.0284\n",
            "Epoch [2, Loss: 0.0287\n",
            "Epoch [2, Loss: 0.0287\n",
            "Epoch [2, Loss: 0.0289\n",
            "Epoch [2, Loss: 0.0289\n",
            "Epoch [2, Loss: 0.0290\n",
            "Epoch [2, Loss: 0.0293\n",
            "Epoch [2, Loss: 0.0294\n",
            "Epoch [2, Loss: 0.0296\n",
            "Epoch [2, Loss: 0.0298\n",
            "Epoch [2, Loss: 0.0299\n",
            "Epoch [2, Loss: 0.0300\n",
            "Epoch [2, Loss: 0.0303\n",
            "Epoch [2, Loss: 0.0303\n",
            "Epoch [2, Loss: 0.0303\n",
            "Epoch [2, Loss: 0.0304\n",
            "Epoch [2, Loss: 0.0306\n",
            "Epoch [2, Loss: 0.0307\n",
            "Epoch [2, Loss: 0.0308\n",
            "Epoch [2, Loss: 0.0313\n",
            "Epoch [2, Loss: 0.0332\n",
            "Epoch [2, Loss: 0.0336\n",
            "Epoch [2, Loss: 0.0337\n",
            "Epoch [2, Loss: 0.0340\n",
            "Epoch [2, Loss: 0.0341\n",
            "Epoch [2, Loss: 0.0342\n",
            "Epoch [2, Loss: 0.0343\n",
            "Epoch [2, Loss: 0.0343\n",
            "Epoch [2, Loss: 0.0344\n",
            "Epoch [2, Loss: 0.0356\n",
            "Epoch [2, Loss: 0.0377\n",
            "Epoch [2, Loss: 0.0436\n",
            "Epoch [2, Loss: 0.0441\n",
            "Epoch [2, Loss: 0.0442\n",
            "Epoch [2, Loss: 0.0443\n",
            "Epoch [2, Loss: 0.0450\n",
            "Epoch [2, Loss: 0.0454\n",
            "Epoch [2, Loss: 0.0455\n",
            "Epoch [2, Loss: 0.0456\n",
            "Epoch [2, Loss: 0.0462\n",
            "Epoch [2, Loss: 0.0463\n",
            "Epoch [2, Loss: 0.0464\n",
            "Epoch [2, Loss: 0.0466\n",
            "Epoch [2, Loss: 0.0468\n",
            "Epoch [2, Loss: 0.0471\n",
            "Epoch [2, Loss: 0.0473\n",
            "Epoch [2, Loss: 0.0474\n",
            "Epoch [2, Loss: 0.0475\n",
            "Epoch [2, Loss: 0.0475\n",
            "Epoch [2, Loss: 0.0478\n",
            "Epoch [2, Loss: 0.0481\n",
            "Epoch [2, Loss: 0.0481\n",
            "Epoch [2, Loss: 0.0484\n",
            "Epoch [2, Loss: 0.0485\n",
            "Epoch [2, Loss: 0.0485\n",
            "Epoch [2, Loss: 0.0487\n",
            "Epoch [2, Loss: 0.0488\n",
            "Epoch [2, Loss: 0.0489\n",
            "Epoch [2, Loss: 0.0494\n",
            "Epoch [2, Loss: 0.0496\n",
            "Epoch [2, Loss: 0.0498\n",
            "Epoch [2, Loss: 0.0499\n",
            "Epoch [2, Loss: 0.0499\n",
            "Epoch [2, Loss: 0.0500\n",
            "Epoch [2, Loss: 0.0503\n",
            "Epoch [2, Loss: 0.0506\n",
            "Epoch [2, Loss: 0.0508\n",
            "Epoch [2, Loss: 0.0509\n",
            "Epoch [2, Loss: 0.0510\n",
            "Epoch [2, Loss: 0.0511\n",
            "Epoch [2, Loss: 0.0512\n",
            "Epoch [2, Loss: 0.0514\n",
            "Epoch [2, Loss: 0.0515\n",
            "Epoch [2, Loss: 0.0515\n",
            "Epoch [2, Loss: 0.0516\n",
            "Epoch [2, Loss: 0.0517\n",
            "Epoch [2, Loss: 0.0517\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "model.eval()  # Set the model to evaluation mode\n",
        "y_pred = []\n",
        "y_true = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for emails, labels in test_loader:\n",
        "        outputs = model(emails)\n",
        "        predicted = (outputs > 0.5).float()\n",
        "        y_pred.extend(predicted.squeeze().tolist())\n",
        "        y_true.extend(labels.tolist())\n",
        "\n",
        "print(f'Accuracy: {accuracy_score(y_true, y_pred):.4f}')\n",
        "print(classification_report(y_true, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6u7rk2JMqw8B",
        "outputId": "8490ea03-18ee-49a4-9607-e535b69ab898"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9565\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.96      0.98      0.97      2913\n",
            "         1.0       0.95      0.90      0.92      1157\n",
            "\n",
            "    accuracy                           0.96      4070\n",
            "   macro avg       0.95      0.94      0.95      4070\n",
            "weighted avg       0.96      0.96      0.96      4070\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(text, model, vectorizer):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        vectorized_text = vectorizer.transform([text]).toarray()\n",
        "        vectorized_text = torch.tensor(vectorized_text, dtype=torch.float32)\n",
        "        output = model(vectorized_text)\n",
        "        prediction = (output > 0.5).float().item()\n",
        "    return 'spam' if prediction == 1 else 'ham'\n",
        "\n",
        "# Test with a custom message\n",
        "test_message = \"Congratulations! You've won a free ticket to the Bahamas. Reply with WIN to claim your prize.\"\n",
        "test_message_1 = \"Hi This is Manideep\"\n",
        "print(predict(test_message, model, vectorizer))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bgoa_E_Kqw3e",
        "outputId": "5efaec6d-89ab-4a30-c547-28aacf799ec3"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0])\n",
            "None\n"
          ]
        }
      ]
    }
  ]
}